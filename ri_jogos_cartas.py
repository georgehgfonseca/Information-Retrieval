# -*- coding: utf-8 -*-
"""ri_jogos_cartas.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NxYRQJpxhgXafVD9452DjDIzwSF1ExaX
"""

#Pacotes necessarios

import math
import pandas as pd
import numpy as np

import os
import uuid
import textract
import copy

#Coleta dos documentos
docs = []
docsp = [] #Cópia dos documentos pré-processados
docs_dic = {}

source_directory = os.path.join(os.getcwd(), "drive/MyDrive/collection")
i = 0
dir_files =  sorted(os.listdir(source_directory))
for process_file in dir_files:
    print(process_file)
    file, extension = os.path.splitext(process_file)
    text = textract.process(os.path.join(source_directory, process_file)).decode('utf-8', 'ignore')
    text = file + "\n" + text
    docs.append(text)
    docs_dic[i] = process_file
    i += 1

#Pre-processamento
print("Documentos preprocessados (docsp):")
k = []
for d in docs:
    #Remoção de pontuação
    d = d.replace(",", "")
    d = d.replace(".", "")
    d = d.replace("!", "")
    d = d.replace("\n", " ")
    d = d.replace("\r", "")
    d = d.strip()
    #Remoção de caixa alta
    d = d.lower()
    #Quebra por palavras
    d = d.split(" ")
    docsp.append(d)
    print("Doc:", d)

#Obtencao dos termos (k)
for d in docsp:
    for palavra in d:
        if palavra not in k:
            k.append(palavra)

print("\nLista de Termos (k): ")
print(k)

#Criando a matriz de termos (fij) e frequência total (Fi) (Tab. 2.1)
f = [[0 for j in docsp] for i in k]
F = [0 for i in k]
N = len(docsp)
for i in range(len(k)):
    aux = 0
    for j in range(N):
        f[i][j] = docsp[j].count(k[i])
        aux += f[i][j]
    F[i] = aux
print("\nMatriz de frequencia de termos (f):")
print(f)

print("\nFrequencia de termos (F):")
print(F)

#Criando o indice invertido (Tab. 2.2)
iinv = [[[t, 0]] for t in k]
for i in range(len(docsp)):
    for p in docsp[i]:
        for t in iinv:
            if t[0][0] == p:
                t[0][1] += 1
                doc_indexado = False
                for j in range(len(t)):
                    (d, freq) = t[j]
                    if d == i:
                        t[j][1] += 1
                        doc_indexado = True
                        break
                if not doc_indexado:
                    t.append([i, 1])
                break
print("\nIndice invertido (iinv):")
for t in iinv:
    print(t)

#Leitura da consulta
c = input("\nDigite uma consulta: ")
c = c.replace(",", "")
c = c.replace(".", "")
c = c.replace("!", "")
d = c.replace("\n", " ")
c = c.strip()
c = c.lower()
c = c.split(" ")
print("\nConsulta considerada:")
print(c)
consultas = [c]

#Modelo booleano
#Computando componenetes conjuntivos de termos
c_doc = [[0 for i in k] for j in docsp]
c_con = [[0 for i in k] for q in consultas]
for i in range(len(k)):
    for j in range(len(docsp)):
        if k[i] in docsp[j]:
            c_doc[j][i] = 1
    for q in range(len(consultas)):
        if k[i] in consultas[q]:
            c_con[q][i] = 1

#Calculando a similaridade no modelo Booleano (eq. 3.3)
simB = [[1 for q in consultas] for j in docsp]
for q in range(len(consultas)):
    for j in range(len(docsp)):
        for i in range(len(k)):
            if c_con[q][i] == 1 and c_doc[j][i] == 0:
                simB[j][q] = 0
print("\nSimilaridade no modelo Booleano (simB):")
print(simB)

#Calculando frequencia de termo logarítimica (TF) (eq. 3.7)
tf = [[0 for j in docsp] for i in k]
for i in range(len(k)):
    for j in range(N):
        if f[i][j] != 0:
            tf[i][j] = 1 + math.log2(f[i][j])
        else:
            tf[i][j] = 0
print("\nFrequencia de termo logarítimica (tf):")
print(tf)

#Calculando frequencia inversa de documentos (IDF) (eq. 3.8)
n = [0 for i in k]
for i in range(len(k)):
    for j in range(N):
        if f[i][j] > 0:
            n[i] += 1
idf = [0 for i in k]
for i in range(len(k)):
    idf[i] = math.log2(N / n[i])
print("\nFrequencia inversa de documentos (idf):")
print(idf)

#Calculando a ponderação TF-IDF (eq. 3.9)
tf_idf = [[0 for j in docsp] for i in k]
for i in range(len(k)):
    for j in range(N):
        if f[i][j] > 0:
            tf_idf[i][j] = (1 + math.log2(f[i][j])) * math.log2(N / n[i])
        else:
            tf_idf[i][j] = 0
w = tf_idf
print("\nPonderacao TF-IDF (tf-idf e w)")
print(w)

#Medidas de normalização do tamanho dos documentos (eq. 3.10)
print("\nMedidas do tamanho dos documentos:")
docs_carac = [len(i) for i in docs]
print("\na. Numero de caracteres (d_carac):")
print(docs_carac)

docsp_pal = [len(i) for i in docsp]
print("\nb. Numero de palavras apos preprocessamento (docsp_pal):")
print(docsp_pal)

docsp_n = [0 for i in docsp]
for j in range(len(docsp)):
    for i in range(len(k)):
        docsp_n[j] += math.pow(w[i][j], 2)
    docsp_n[j] = math.sqrt(docsp_n[j])
print("\nc. Normas dos documentos (docsp_n):")
print(docsp_n)

#Computando fiq e wiq das consultas (eq. 3.13)
f_cons = [[0 for q in consultas] for i in k]
w_cons = [[0 for q in consultas] for i in k]
for i in range(len(k)):
    for q in range(len(consultas)):
        f_cons[i][q] = consultas[q].count(k[i])
        if f_cons[i][q] > 0:
            w_cons[i][q] = (1 + math.log2(f_cons[i][q])) * math.log2(N / n[i])
        else:
            w_cons[i][q] = 0
print("\nTF-IDF dos termos da consulta (w_cons):")
print(w_cons)

#Calculando a similaridade entre documento dj e consulta q (modelo vetorial) (eq. 3.12)
simV = [[0 for q in consultas] for j in docsp]
for j in range(len(docsp)):
    for q in range(len(consultas)):
        num = 0
        for i in range(len(k)):
            num += w[i][j] * w_cons[i][q]
        den = 0
        den1 = 0
        den2 = 0
        for i in range(len(k)):
            den1 += math.pow(w[i][j], 2)
            den2 += math.pow(w_cons[i][q], 2)
        den1 = math.sqrt(den1)
        den2 = math.sqrt(den2)
        den = den1 * den2
        if den != 0:
            simV[j][q] = num / den
print("\nSimilaridade no modelo Vetorial (simV):")
print(np.array(simV))

#Modelo Probabilístico (eq. 3.24)
simP = [[0 for q in consultas] for j in docsp]
for i in range(len(k)):
    for q in range(len(consultas)):
        for j in range(len(docsp)):
            if k[i] in consultas[q] and k[i] in docsp[j]:
                simP[j][q] += math.log2((N + 0.5) / (n[i] + 0.5))
print("\nSimilaridade no modelo Probabilistico (simP):")
print(np.array(simP))

#Apresentando resultados da recuperacao (simV)
sim = copy.deepcopy(simV) #Copia do vetor de similaridades (simV)
n = 0                     #Numero de documentos recuperados
docs_rec = {}             #Documentos recuperados
max_rec = 5               #Maximo de documentos recuperados exibidos
print("\nApresentacao de ate %d dos melhores resultados (modelo vetorial)" % max_rec)
print("================================================================================")
print("| Pos  |                                              Documento | Similaridade |")
for i in range(0, max_rec):
    if max(sim) == [0]:
        print("|                   Nao foram recuperados mais documentos.                     |")
        break
    idx = sim.index(max(sim))
    doc = docs_dic[sim.index(max(sim))]
    print("|%4d. |%55s |%13.3f |" % (i+1, docs_dic[idx], sim[idx][0]))
    docs_rec[idx] = doc
    sim[idx][0] = 0
    n += 1
print("================================================================================")

#Avaliando a recuperacao (modelo Vetorial)
#Documentos avaliados (por um humano) como relevantes: 0, 9, 8, 1, 3, 2
docs_rel = {}
docs_rel[0] = docs_dic[0]
docs_rel[9] = docs_dic[9]
docs_rel[8] = docs_dic[8]
docs_rel[1] = docs_dic[1]
docs_rel[3] = docs_dic[3]
docs_rel[2] = docs_dic[2]

pre_num = 0
rev_num = 0
pre_den = 0
rev_den = len(docs_rel)
docs_rel_pre = {}
prec = []
rev = []
rec_keys = list(docs_rec.keys())
rel_keys = list(docs_rel.keys())
print("\nAvaliacao da recuperacao (modelo vetorial)")
print("Documentos recuperados (indices): ", rec_keys)
print("Documentos relevantes (indices):  ", rel_keys)
print("\n==============================")
print("| Pos | Precisao | Revocacao |")
for i in range(0, n):
    if i < len(rec_keys):
        pre_den += 1
        if rec_keys[i] in rel_keys: #Documento recuperado e relevante
            pre_num += 1
            rev_num += 1
            docs_rel_pre[rec_keys[i]] = pre_num / pre_den
    prec.append(pre_num / pre_den)
    rev.append(rev_num / rev_den)
    print("| %3d |  %2d / %2d |  %2d / %2d  |" % (i + 1, pre_num, pre_den, rev_num, rev_den))
print("==============================")

#Media das precisoes medias (map) (eq. 4.3)
print("\nPrecisao dos documentos relevantes (P(Ri[k])): ", docs_rel_pre)
soma = 0
for i in docs_rel:
    if i in docs_rel_pre:
        soma += docs_rel_pre[i]

map = soma / len(docs_rel)
print("\nMedia das precisoes medias:")
print(map)

#Precisao interpolada (eq. 47)
rj = 5 #Padrao de revocacao 5 (recupera pelo menos 50% dos documentos relevantes)
prec_max_rj = -float("inf")
for i in range(len(prec)):
    if prec[i] > prec_max_rj and rev[i] >= rj / 10:
        prec_max_rj = prec[i]
print("\nPrecisao interpolada no %d%%-esimo nivel padrao de revocacao:" % (rj * 10))
print(prec_max_rj)

#F-score (eq. 4.9)
pos = 4
fScore = 0
if len(rev) >= pos + 1:
    if prec[pos] != 0 and rev[pos] != 0:
        fScore = 2 / ((1 / rev[pos]) + (1 / prec[pos]))
print("\nF-Score (pos: %d):" % (pos + 1))
print(fScore)